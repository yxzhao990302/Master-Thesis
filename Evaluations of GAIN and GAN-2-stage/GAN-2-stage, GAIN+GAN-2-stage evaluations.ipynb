{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693aef90-f685-4379-90b8-f79b3fb0a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wang\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a9586-5172-43e5-97ea-150c47b31172",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736680f9-604c-4f27-a729-8138f789c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"concat_v2\" in dir(tf):\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return tf.concat_v2(tensors, axis, *args, **kwargs)\n",
    "else:\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return tf.concat(tensors, axis, *args, **kwargs)\n",
    "\n",
    "def bn(x, is_training, scope):\n",
    "    return tf.compat.v1.layers.batch_normalization(x,\n",
    "                                        momentum=0.9,\n",
    "                                        epsilon=1e-5,\n",
    "                                        scale=True)\n",
    "\n",
    "def conv_out_size_same(size, stride):\n",
    "    return int(math.ceil(float(size) / float(stride)))\n",
    "\n",
    "def conv_cond_concat(x, y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis.\"\"\"\n",
    "    x_shapes = x.get_shape()\n",
    "    y_shapes = y.get_shape()\n",
    "    return concat([x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)\n",
    "\n",
    "def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "              initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "\n",
    "        return conv\n",
    "\n",
    "def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, name=\"deconv2d\", stddev=0.02, with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        # filter : [height, width, output_channels, in_channels]\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        # Support for verisons of TensorFlow before 0.7.0\n",
    "        except AttributeError:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv\n",
    "\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    return tf.maximum(x, leak*x)\n",
    "\n",
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "        initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18287b95-8581-45c2-b651-c4e2dbdce78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.python.ops.rnn_cell_impl import LayerRNNCell\n",
    "from tensorflow.python.layers import base as base_layer\n",
    "from tensorflow.python.ops import nn_ops\n",
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "\n",
    "class MyGRUCell15(LayerRNNCell):\n",
    "  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n",
    "\n",
    "  Args:\n",
    "    num_units: int, The number of units in the GRU cell.\n",
    "    activation: Nonlinearity to use.  Default: `tanh`.\n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "    name: String, the name of the layer. Layers with the same name willbn\n",
    "      share weights, but to avoid mistakes we require reuse=True in such\n",
    "      cases.\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None,\n",
    "               name=None):\n",
    "    super(MyGRUCell15, self).__init__(_reuse=reuse, name=name)\n",
    "\n",
    "    # Inputs must be 2-dimensional.\n",
    "    self.input_spec = base_layer.InputSpec(ndim=2)\n",
    "\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def build(self, inputs_shape):\n",
    "    if inputs_shape[1].value is None:\n",
    "      raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n",
    "                       % inputs_shape)\n",
    "\n",
    "    input_depth = inputs_shape[1].value-self._num_units\n",
    "    self._gate_kernel = self.add_variable(\n",
    "        \"gates/%s\" % _WEIGHTS_VARIABLE_NAME,\n",
    "        shape=[input_depth + self._num_units, 2 * self._num_units],\n",
    "        initializer=self._kernel_initializer)\n",
    "    self._gate_bias = self.add_variable(\n",
    "        \"gates/%s\" % _BIAS_VARIABLE_NAME,\n",
    "        shape=[2 * self._num_units],\n",
    "        initializer=(\n",
    "            self._bias_initializer\n",
    "            if self._bias_initializer is not None\n",
    "            else tf.constant_initializer(1.0, dtype=self.dtype)))\n",
    "    self._candidate_kernel = self.add_variable(\n",
    "        \"candidate/%s\" % _WEIGHTS_VARIABLE_NAME,\n",
    "        shape=[input_depth + self._num_units, self._num_units],\n",
    "        initializer=self._kernel_initializer)\n",
    "    self._candidate_bias = self.add_variable(\n",
    "        \"candidate/%s\" % _BIAS_VARIABLE_NAME,\n",
    "        shape=[self._num_units],\n",
    "        initializer=(\n",
    "            self._bias_initializer\n",
    "            if self._bias_initializer is not None\n",
    "            else tf.zeros_initializer(dtype=self.dtype)))\n",
    "\n",
    "    self.built = True\n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "    totalLength=inputs.get_shape().as_list()[1]\n",
    "    inputs_=inputs[:,0:totalLength-self._num_units]\n",
    "    rth=inputs[:,totalLength-self._num_units:]\n",
    "    inputs=inputs_\n",
    "    state=math_ops.multiply(rth,state)\n",
    "\n",
    "    gate_inputs = math_ops.matmul(\n",
    "        array_ops.concat([inputs, state], 1), self._gate_kernel)\n",
    "    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)\n",
    "\n",
    "    value = math_ops.sigmoid(gate_inputs)\n",
    "    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "    r_state = r * state\n",
    "\n",
    "    candidate = math_ops.matmul(\n",
    "        array_ops.concat([inputs, r_state], 1), self._candidate_kernel)\n",
    "    candidate = nn_ops.bias_add(candidate, self._candidate_bias)\n",
    "\n",
    "    c = self._activation(candidate)\n",
    "    new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f929b9-d3dd-4279-ab2e-574118c34081",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WGAN(object):\n",
    "    model_name = \"WGAN_no_mask\"     # name for checkpoint\n",
    "\n",
    "    def __init__(self, sess, args, datasets):\n",
    "        self.sess = sess\n",
    "        self.isbatch_normal=args.isBatch_normal\n",
    "        self.isNormal=args.isNormal\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        self.result_dir = args.result_dir\n",
    "        self.log_dir = args.log_dir\n",
    "        self.dataset_name=args.dataset_name\n",
    "        self.run_type=args.run_type\n",
    "        self.lr = args.lr                 \n",
    "        self.epoch = args.epoch     \n",
    "        self.batch_size = args.batch_size\n",
    "        self.n_inputs = args.n_inputs                 # MNIST data input (img shape: 28*28)\n",
    "        self.n_steps = datasets.maxLength                                # time steps\n",
    "        self.n_hidden_units = args.n_hidden_units        # neurons in hidden layer\n",
    "        self.n_classes = args.n_classes                # MNIST classes (0-9 digits)\n",
    "        self.gpus=args.gpus\n",
    "        self.run_type=args.run_type\n",
    "        self.result_path=args.result_path\n",
    "        self.model_path=args.model_path\n",
    "        self.pretrain_epoch=args.pretrain_epoch\n",
    "        self.impute_iter=args.impute_iter\n",
    "        self.isSlicing=args.isSlicing\n",
    "        self.g_loss_lambda=args.g_loss_lambda\n",
    "        \n",
    "        self.datasets=datasets\n",
    "        self.z_dim = args.z_dim         # dimension of noise-vector\n",
    "        self.gen_length=args.gen_length\n",
    "        \n",
    "        # WGAN_GP parameter\n",
    "        self.lambd = 0.25       # The higher value, the more stable, but the slower convergence\n",
    "        self.disc_iters = args.disc_iters     # The number of critic iterations for one-step of generator\n",
    "\n",
    "        # train\n",
    "        self.learning_rate = args.lr\n",
    "        self.beta1 = args.beta1\n",
    "    \n",
    "        self.grud_cell_d = MyGRUCell15(self.n_hidden_units)\n",
    "        self.grud_cell_g = MyGRUCell15(self.n_hidden_units)\n",
    "        \n",
    "        self.sample_num = 64  # number of generated images to be saved\n",
    "\n",
    "        self.num_batches = len(datasets.x) // self.batch_size\n",
    "\n",
    "      \n",
    "    def pretrainG(self, X, M, Delta,  Mean, Lastvalues, X_lengths, Keep_prob, is_training=True, reuse=False):\n",
    "        \n",
    "        with tf.variable_scope(\"g_enerator\", reuse=reuse):\n",
    "            \n",
    "            \"\"\"\n",
    "            the rnn cell's variable scope is defined by tensorflow,\n",
    "            if we want to update rnn cell's weights, the variable scope must contains 'g_' or 'd_'\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            wr_h=tf.get_variable(\"g_wr_h\",shape=[self.n_inputs,self.n_hidden_units],initializer=tf.random_normal_initializer())\n",
    "            w_out= tf.get_variable(\"g_w_out\",shape=[self.n_hidden_units, self.n_inputs],initializer=tf.random_normal_initializer())\n",
    "            \n",
    "            br_h= tf.get_variable(\"g_br_h\",shape=[self.n_hidden_units, ],initializer=tf.constant_initializer(0.001))\n",
    "            b_out= tf.get_variable(\"g_b_out\",shape=[self.n_inputs, ],initializer=tf.constant_initializer(0.001))\n",
    "            w_z=tf.get_variable(\"g_w_z\",shape=[self.z_dim,self.n_inputs],initializer=tf.random_normal_initializer())\n",
    "            b_z=tf.get_variable(\"g_b_z\",shape=[self.n_inputs, ],initializer=tf.constant_initializer(0.001))\n",
    "            \n",
    "            \n",
    "            X = tf.reshape(X, [-1, self.n_inputs])\n",
    "            Delta=tf.reshape(Delta,[-1,self.n_inputs])\n",
    "            \n",
    "            rth= tf.matmul(Delta, wr_h)+br_h\n",
    "            rth=math_ops.exp(-tf.maximum(0.0,rth))\n",
    "            \n",
    "            X=tf.concat([X,rth],1)\n",
    "            \n",
    "            X_in = tf.reshape(X, [-1, self.n_steps, self.n_inputs+self.n_hidden_units])\n",
    "         \n",
    "            init_state = self.grud_cell_g.zero_state(self.batch_size, dtype=tf.float32) \n",
    "            outputs, final_state = tf.nn.dynamic_rnn(self.grud_cell_g, X_in, \\\n",
    "                                initial_state=init_state,\\\n",
    "                                sequence_length=X_lengths,\n",
    "                                time_major=False)\n",
    "            #outputs: batch_size*n_steps*n_hiddensize\n",
    "            outputs=tf.reshape(outputs,[-1,self.n_hidden_units])\n",
    "            out_predict=tf.matmul(tf.nn.dropout(outputs,Keep_prob), w_out) + b_out\n",
    "            out_predict=tf.reshape(out_predict,[-1,self.n_steps,self.n_inputs])\n",
    "            return out_predict\n",
    "\n",
    "\n",
    "    def discriminator(self, X, M, DeltaPre, Lastvalues ,DeltaSub ,SubValues , Mean,  X_lengths,Keep_prob, is_training=True, reuse=False, isTdata=True):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "        with tf.variable_scope(\"d_iscriminator\", reuse=reuse):\n",
    "            \n",
    "            wr_h=tf.get_variable(\"d_wr_h\",shape=[self.n_inputs,self.n_hidden_units],initializer=tf.random_normal_initializer())\n",
    "            w_out= tf.get_variable(\"d_w_out\",shape=[self.n_hidden_units, 1],initializer=tf.random_normal_initializer())\n",
    "            br_h= tf.get_variable(\"d_br_h\",shape=[self.n_hidden_units, ],initializer=tf.constant_initializer(0.001))\n",
    "            b_out= tf.get_variable(\"d_b_out\",shape=[1, ],initializer=tf.constant_initializer(0.001))\n",
    "          \n",
    "           \n",
    "            M=tf.reshape(M,[-1,self.n_inputs])\n",
    "            X = tf.reshape(X, [-1, self.n_inputs])\n",
    "            DeltaPre=tf.reshape(DeltaPre,[-1,self.n_inputs])\n",
    "           \n",
    "            # rth: time decay vector\n",
    "            rth= tf.matmul(DeltaPre, wr_h)+br_h\n",
    "            rth=math_ops.exp(-tf.maximum(0.0,rth))\n",
    "            # add noise\n",
    "            #X=X+np.random.standard_normal(size=(self.batch_size*self.n_steps, self.n_inputs))/100 \n",
    "            X=tf.concat([X,rth],1)\n",
    "              \n",
    "            X_in = tf.reshape(X, [self.batch_size, self.n_steps , self.n_inputs+self.n_hidden_units])\n",
    "            \n",
    "            init_state = self.grud_cell_d.zero_state(self.batch_size, dtype=tf.float32) \n",
    "            outputs, final_state = tf.nn.dynamic_rnn(self.grud_cell_d, X_in, \\\n",
    "                                initial_state=init_state,\\\n",
    "                                sequence_length=X_lengths,\n",
    "                                time_major=False)\n",
    "         \n",
    "           \n",
    "            out_logit=tf.matmul(tf.nn.dropout(final_state,Keep_prob), w_out) + b_out\n",
    "            out =tf.nn.sigmoid(out_logit)   \n",
    "            return out,out_logit\n",
    "\n",
    "    def generator(self, z, Keep_prob, is_training=True, reuse=False):\n",
    "        # x,delta,n_steps\n",
    "        # z :[self.batch_size, self.z_dim]\n",
    "        # first feed noize in rnn, then feed the previous output into next input\n",
    "        # or we can feed noize and previous output into next input in future version\n",
    "        with tf.variable_scope(\"g_enerator\", reuse=reuse):\n",
    "            #gennerate \n",
    "            \n",
    "            wr_h=tf.get_variable(\"g_wr_h\",shape=[self.n_inputs,self.n_hidden_units],initializer=tf.random_normal_initializer())\n",
    "            w_out= tf.get_variable(\"g_w_out\",shape=[self.n_hidden_units, self.n_inputs],initializer=tf.random_normal_initializer())\n",
    "            br_h= tf.get_variable(\"g_br_h\",shape=[self.n_hidden_units, ],initializer=tf.constant_initializer(0.001))\n",
    "            b_out= tf.get_variable(\"g_b_out\",shape=[self.n_inputs, ],initializer=tf.constant_initializer(0.001))\n",
    "            w_z=tf.get_variable(\"g_w_z\",shape=[self.z_dim,self.n_inputs],initializer=tf.random_normal_initializer())\n",
    "            b_z=tf.get_variable(\"g_b_z\",shape=[self.n_inputs, ],initializer=tf.constant_initializer(0.001))\n",
    "            \n",
    "            #self.times=tf.reshape(self.times,[self.batch_size,self.n_steps,self.n_inputs])\n",
    "            #change z's dimension\n",
    "            # batch_size*z_dim-->batch_size*n_inputs\n",
    "            x=tf.matmul(z,w_z)+b_z\n",
    "            x=tf.reshape(x,[-1,self.n_inputs])\n",
    "            delta_zero=tf.constant(0.0,shape=[self.batch_size,self.n_inputs])\n",
    "          \n",
    "            \n",
    "\n",
    "            # combine X_in\n",
    "            rth= tf.matmul(delta_zero, wr_h)+br_h\n",
    "            rth=math_ops.exp(-tf.maximum(0.0,rth))\n",
    "            x=tf.concat([x,rth],1)\n",
    "            \n",
    "            X_in = tf.reshape(x, [-1, 1, self.n_inputs+self.n_hidden_units])\n",
    "            \n",
    "            init_state = self.grud_cell_g.zero_state(self.batch_size, dtype=tf.float32) # 初始化全零 state\n",
    "            #z=tf.reshape(z,[self.batch_size,1,self.z_dim])\n",
    "            seq_len=tf.constant(1,shape=[self.batch_size])\n",
    "            \n",
    "            outputs, final_state = tf.nn.dynamic_rnn(self.grud_cell_g, X_in, \\\n",
    "                                initial_state=init_state,\\\n",
    "                                sequence_length=seq_len,\n",
    "                                time_major=False)\n",
    "            init_state=final_state\n",
    "            #outputs: batch_size*1*n_hidden\n",
    "            outputs=tf.reshape(outputs,[-1,self.n_hidden_units])\n",
    "            # full connect\n",
    "            out_predict=tf.matmul(tf.nn.dropout(outputs,Keep_prob), w_out) + b_out\n",
    "            out_predict=tf.reshape(out_predict,[-1,1,self.n_inputs])\n",
    "            \n",
    "            total_result=tf.multiply(out_predict,1.0)\n",
    "            \n",
    "            for i in range(1,self.n_steps):\n",
    "                out_predict=tf.reshape(out_predict,[self.batch_size,self.n_inputs])\n",
    "               \n",
    "                delta_normal=tf.reshape(self.imputed_deltapre[:,i:(i+1),:],[self.batch_size,self.n_inputs])\n",
    "                rth= tf.matmul(delta_normal, wr_h)+br_h\n",
    "                rth=math_ops.exp(-tf.maximum(0.0,rth))\n",
    "                x=tf.concat([out_predict,rth],1)\n",
    "                X_in = tf.reshape(x, [-1, 1, self.n_inputs+self.n_hidden_units])\n",
    "                \n",
    "                outputs, final_state = tf.nn.dynamic_rnn(self.grud_cell_g, X_in, \\\n",
    "                            initial_state=init_state,\\\n",
    "                            sequence_length=seq_len,\n",
    "                            time_major=False)\n",
    "                init_state=final_state\n",
    "                outputs=tf.reshape(outputs,[-1,self.n_hidden_units])\n",
    "                out_predict=tf.matmul(tf.nn.dropout(outputs,Keep_prob), w_out) + b_out\n",
    "                out_predict=tf.reshape(out_predict,[-1,1,self.n_inputs])\n",
    "                total_result=tf.concat([total_result,out_predict],1)\n",
    "            \n",
    "           \n",
    "        \n",
    "            if self.isbatch_normal:\n",
    "                with tf.variable_scope(\"g_bn\", reuse=tf.AUTO_REUSE):\n",
    "                    total_result=bn(total_result,is_training=is_training, scope=\"g_bn_imple\")\n",
    "            \n",
    "            \n",
    "            last_values=tf.multiply(total_result,1)\n",
    "            sub_values=tf.multiply(total_result,1)\n",
    "\n",
    "            return total_result,self.imputed_deltapre,self.imputed_deltasub,self.imputed_m,self.x_lengths,last_values,sub_values\n",
    "        \n",
    "    def impute(self):\n",
    "        with tf.variable_scope(\"impute\", reuse=tf.AUTO_REUSE):\n",
    "            z_need_tune=tf.get_variable(\"z_needtune\",shape=[self.batch_size,self.z_dim],initializer=tf.random_normal_initializer(mean=0,stddev=0.1) )\n",
    "            return z_need_tune\n",
    "            \n",
    "    def build_model(self):\n",
    "        \n",
    "        self.keep_prob = tf.placeholder(tf.float32) \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_classes])\n",
    "        self.m = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.mean = tf.placeholder(tf.float32, [self.n_inputs,])\n",
    "        self.deltaPre = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.lastvalues = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.deltaSub = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.subvalues = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.x_lengths = tf.placeholder(tf.int32,  shape=[self.batch_size,])\n",
    "        self.imputed_deltapre=tf.placeholder(tf.float32,  shape=[self.batch_size,self.n_steps,self.n_inputs])\n",
    "        self.imputed_deltasub=tf.placeholder(tf.float32,  shape=[self.batch_size,self.n_steps,self.n_inputs])\n",
    "        self.imputed_m = tf.placeholder(tf.float32, [None, self.n_steps, self.n_inputs])\n",
    "        self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim], name='z')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "\n",
    "        Pre_out=self.pretrainG(self.x, self.m, self.deltaPre,  self.mean,\\\n",
    "                                                      self.lastvalues, self.x_lengths,self.keep_prob, \\\n",
    "                                                      is_training=True, reuse=False)\n",
    "        \n",
    "        self.pretrain_loss=tf.reduce_sum(tf.square(tf.multiply(Pre_out,self.m)-self.x)) / tf.cast(tf.reduce_sum(self.x_lengths),tf.float32)\n",
    "        \n",
    "        #discriminator( X, M, DeltaPre, Lastvalues ,DeltaSub ,SubValues , Mean,  X_lengths,Keep_prob, is_training=True, reuse=False, isTdata=True):\n",
    "        \n",
    "        D_real, D_real_logits = self.discriminator(self.x, self.m, self.deltaPre,self.lastvalues,\\\n",
    "                                                   self.deltaSub,self.subvalues,  self.mean,\\\n",
    "                                                       self.x_lengths,self.keep_prob, \\\n",
    "                                                      is_training=True, reuse=False,isTdata=True)\n",
    "\n",
    "        #G return total_result,self.imputed_deltapre,self.imputed_deltasub,self.imputed_m,self.x_lengths,last_values,sub_values\n",
    "        g_x,g_deltapre,g_deltasub,g_m,G_x_lengths,g_last_values,g_sub_values = self.generator(self.z,self.keep_prob, is_training=True, reuse=True)\n",
    "        \n",
    "        D_fake, D_fake_logits = self.discriminator(g_x,g_m,g_deltapre,g_last_values,\\\n",
    "                                                   g_deltasub,g_sub_values,self.mean,\\\n",
    "                                                      G_x_lengths,self.keep_prob,\n",
    "                                                      is_training=True, reuse=True ,isTdata=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        impute loss\n",
    "        \"\"\"\n",
    "        self.z_need_tune=self.impute()\n",
    "        \n",
    "        impute_out,impute_deltapre,impute_deltasub,impute_m,impute_x_lengths,impute_last_values,impute_sub_values=self.generator(self.z_need_tune,self.keep_prob, is_training=False, reuse=True)\n",
    "        \n",
    "        \n",
    "        impute_fake, impute_fake_logits = self.discriminator(impute_out,impute_m,impute_deltapre,impute_last_values,\\\n",
    "                                                             impute_deltasub,impute_sub_values,self.mean,\\\n",
    "                                                      impute_x_lengths,self.keep_prob,\n",
    "                                                      is_training=False, reuse=True ,isTdata=False)\n",
    "        \n",
    "        # loss for imputation\n",
    "        \n",
    "        self.impute_loss=tf.reduce_mean(tf.square(tf.multiply(impute_out,self.m)-self.x))-self.g_loss_lambda*tf.reduce_mean(impute_fake_logits)\n",
    "        \n",
    "        self.impute_out=impute_out\n",
    "        \n",
    "        #the imputed results\n",
    "        self.imputed=tf.multiply((1-self.m),self.impute_out)+self.x\n",
    "        # get loss for discriminator\n",
    "        d_loss_real = - tf.reduce_mean(D_real_logits)\n",
    "        d_loss_fake = tf.reduce_mean(D_fake_logits)\n",
    "\n",
    "\n",
    "        self.d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # get loss for generator\n",
    "        self.g_loss = - d_loss_fake\n",
    "        \n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "        # divide trainable variables into a group for D and a group for G\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "        g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "        z_vars = [self.z_need_tune]\n",
    "        '''\n",
    "        print(\"d vars:\")\n",
    "        for v in d_vars:\n",
    "            print(v.name)\n",
    "        print(\"g vars:\")\n",
    "        for v in g_vars:\n",
    "            print(v.name)\n",
    "        print(\"z vars:\")\n",
    "        for v in z_vars:\n",
    "            print(v.name)\n",
    "        '''\n",
    "        \n",
    "        #don't need normalization because we have adopted the dropout\n",
    "        \"\"\"\n",
    "        ld = 0.0\n",
    "        for w in d_vars:\n",
    "            ld += tf.contrib.layers.l2_regularizer(1e-4)(w)\n",
    "        lg = 0.0\n",
    "        for w in g_vars:\n",
    "            lg += tf.contrib.layers.l2_regularizer(1e-4)(w)\n",
    "        \n",
    "        self.d_loss+=ld\n",
    "        self.g_loss+=lg\n",
    "        \"\"\"\n",
    "        \n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        # this code have used batch normalization, so the upside line should be executed\n",
    "            self.d_optim = tf.train.AdamOptimizer(self.learning_rate, beta1=self.beta1) \\\n",
    "                        .minimize(self.d_loss, var_list=d_vars)\n",
    "            #self.d_optim=self.optim(self.learning_rate, self.beta1,self.d_loss,d_vars)\n",
    "            self.g_optim = tf.train.AdamOptimizer(self.learning_rate*self.disc_iters, beta1=self.beta1) \\\n",
    "                        .minimize(self.g_loss, var_list=g_vars)\n",
    "            #self.g_optim=self.optim(self.learning_rate, self.beta1,self.g_loss,g_vars)\n",
    "            self.g_pre_optim=tf.train.AdamOptimizer(self.learning_rate*2,beta1=self.beta1) \\\n",
    "                        .minimize(self.pretrain_loss,var_list=g_vars)\n",
    "        self.impute_optim=tf.train.AdamOptimizer(self.learning_rate*7,beta1=self.beta1) \\\n",
    "                    .minimize(self.impute_loss,var_list=z_vars)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "        #clip weight\n",
    "        self.clip_all_vals = [p.assign(tf.clip_by_value(p, -0.99, 0.99)) for p in t_vars]\n",
    "        self.clip_D = [p.assign(tf.clip_by_value(p, -0.99, 0.99)) for p in d_vars]\n",
    "        self.clip_G = [p.assign(tf.clip_by_value(p, -0.99, 0.99)) for p in g_vars]\n",
    "        \n",
    "        \n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.fake_x,self.fake_delta,_,_,_,_,_ = self.generator(self.z, self.keep_prob, is_training=False, reuse=True)\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        g_pretrain_loss_sum=tf.summary.scalar(\"g_pretrain_loss\", self.pretrain_loss)\n",
    "        # final summary operations\n",
    "        self.impute_sum=tf.summary.scalar(\"impute_loss\", self.impute_loss)\n",
    "        self.g_sum = g_loss_sum\n",
    "        self.g_pretrain_sum=tf.summary.merge([g_pretrain_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([d_loss_real_sum,d_loss_fake_sum, d_loss_sum])\n",
    "        \n",
    "    def optim(self,learning_rate,beta,loss,var):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta)\n",
    "        grads = optimizer.compute_gradients(loss,var_list=var)\n",
    "        for i, (g, v) in enumerate(grads):\n",
    "            if g is not None:\n",
    "                grads[i] = (tf.clip_by_norm(g, 5), v)  # clip gradients\n",
    "        train_op = optimizer.apply_gradients(grads)\n",
    "        return train_op\n",
    "    def pretrain(self, start_epoch,counter,start_time):\n",
    "        \n",
    "        if start_epoch < self.pretrain_epoch:\n",
    "            #todo\n",
    "            for epoch in range(start_epoch, self.pretrain_epoch):\n",
    "            # get batch data\n",
    "                self.datasets.shuffle(self.batch_size,True)\n",
    "                idx=0\n",
    "                #x,y,mean,m,deltaPre,x_lengths,lastvalues,files,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub\n",
    "                for data_x,data_y,data_mean,data_m,data_deltaPre,data_x_lengths,data_lastvalues,_,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub in self.datasets.nextBatch():\n",
    "                    \n",
    "                    # pretrain\n",
    "                    _, summary_str, p_loss = self.sess.run([self.g_pre_optim, self.g_pretrain_sum, self.pretrain_loss],\n",
    "                                                   feed_dict={self.x: data_x,\n",
    "                                                              self.m: data_m,\n",
    "                                                              self.deltaPre: data_deltaPre,\n",
    "                                                              self.mean: data_mean,\n",
    "                                                              self.x_lengths: data_x_lengths,\n",
    "                                                              self.lastvalues: data_lastvalues,\n",
    "                                                              self.deltaSub:deltaSub,\n",
    "                                                              self.subvalues:subvalues,\n",
    "                                                              self.imputed_m:imputed_m,\n",
    "                                                              self.imputed_deltapre:imputed_deltapre,\n",
    "                                                              self.imputed_deltasub:imputed_deltasub,\n",
    "                                                              self.keep_prob: 0.5})\n",
    "                    self.writer.add_summary(summary_str, counter)\n",
    "    \n",
    "    \n",
    "                    counter += 1\n",
    "    \n",
    "                    # display training status\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, pretrain_loss: %.8f\" \\\n",
    "                          % (epoch, idx, self.num_batches, time.time() - start_time, p_loss))\n",
    "                    idx+=1\n",
    "               \n",
    "                start_batch_id = 0\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.standard_normal(size=(self.batch_size , self.z_dim))\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name+'/'+self.model_dir)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            #start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            start_batch_id=0\n",
    "            #counter = checkpoint_counter\n",
    "            counter=start_epoch*self.num_batches\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "            return \n",
    "        else:\n",
    "            # initialize all variables\n",
    "            tf.global_variables_initializer().run()\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.pretrain(start_epoch,counter,start_time)\n",
    "        if start_epoch < self.pretrain_epoch:\n",
    "            start_epoch=self.pretrain_epoch\n",
    "        \n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            # get batch data\n",
    "            self.datasets.shuffle(self.batch_size,True)\n",
    "            idx=0\n",
    "            for data_x,data_y,data_mean,data_m,data_deltaPre,data_x_lengths,data_lastvalues,_,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub in self.datasets.nextBatch():\n",
    "                \n",
    "                batch_z = np.random.standard_normal(size=(self.batch_size, self.z_dim))\n",
    "                #_ = self.sess.run(self.clip_D)\n",
    "                _ = self.sess.run(self.clip_all_vals)\n",
    "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss],\n",
    "                                               feed_dict={self.z: batch_z,\n",
    "                                                          self.x: data_x,\n",
    "                                                          self.m: data_m,\n",
    "                                                          self.deltaPre: data_deltaPre,\n",
    "                                                          self.mean: data_mean,\n",
    "                                                          self.x_lengths: data_x_lengths,\n",
    "                                                          self.lastvalues: data_lastvalues,\n",
    "                                                          self.deltaSub:deltaSub,\n",
    "                                                          self.subvalues:subvalues,\n",
    "                                                          self.imputed_m:imputed_m,\n",
    "                                                          self.imputed_deltapre:imputed_deltapre,\n",
    "                                                          self.imputed_deltasub:imputed_deltasub,\n",
    "                                                          self.keep_prob: 0.5})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # update G network\n",
    "                if counter%self.disc_iters==0:\n",
    "                    #batch_z = np.random.normal(0, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "                    _, summary_str, g_loss = self.sess.run([self.g_optim, self.g_sum, self.g_loss], \n",
    "                                                           feed_dict={self.z: batch_z,\n",
    "                                                           self.keep_prob: 0.5,\n",
    "                                                           self.deltaPre: data_deltaPre,\n",
    "                                                           self.mean: data_mean,\n",
    "                                                           self.x_lengths: data_x_lengths,\n",
    "                                                           self.lastvalues: data_lastvalues,\n",
    "                                                           self.deltaSub:deltaSub,\n",
    "                                                           self.subvalues:subvalues,\n",
    "                                                           self.imputed_m:imputed_m,\n",
    "                                                           self.imputed_deltapre:imputed_deltapre,\n",
    "                                                           self.imputed_deltasub:imputed_deltasub,\n",
    "                                                           self.mean: data_mean})\n",
    "                    self.writer.add_summary(summary_str, counter)\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f,counter:%4d\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss,counter))\n",
    "                    #debug \n",
    "\n",
    "                counter += 1\n",
    "\n",
    "                # display training status\n",
    "                print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, counter:%4d\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, counter))\n",
    "\n",
    "                # save training results for every 300 steps\n",
    "                if np.mod(counter, 300) == 0 :\n",
    "                    fake_x,fake_delta = self.sess.run([self.fake_x,self.fake_delta],\n",
    "                                            feed_dict={self.z: batch_z,\n",
    "                                                       self.deltaPre: data_deltaPre,\n",
    "                                                       self.mean: data_mean,\n",
    "                                                       self.x_lengths: data_x_lengths,\n",
    "                                                       self.lastvalues: data_lastvalues,\n",
    "                                                       self.deltaSub:deltaSub,\n",
    "                                                       self.subvalues:subvalues,\n",
    "                                                       self.imputed_m:imputed_m,\n",
    "                                                       self.imputed_deltapre:imputed_deltapre,\n",
    "                                                       self.imputed_deltasub:imputed_deltasub,\n",
    "                                                       self.mean: data_mean,\n",
    "                                                       self.keep_prob: 0.5})\n",
    "                    if self.run_type==\"train\":\n",
    "                        self.writeG_Samples(\"G_sample_x\",counter,fake_x)\n",
    "                        self.writeG_Samples(\"G_sample_delta\",counter,fake_delta)\n",
    "                    \n",
    "                idx+=1\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "        \n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "    def imputation(self,dataset,isTrain):\n",
    "        self.datasets=dataset\n",
    "        self.datasets.shuffle(self.batch_size,True)\n",
    "        tf.variables_initializer([self.z_need_tune]).run()\n",
    "       \n",
    "        start_time = time.time()\n",
    "        batchid=1\n",
    "        impute_tune_time=1\n",
    "        counter=1\n",
    "        for data_x,data_y,data_mean,data_m,data_deltaPre,data_x_lengths,data_lastvalues,_,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub in self.datasets.nextBatch():\n",
    "            \n",
    "            \n",
    "            np.save(\"588/batch\"+str(batchid)+\"x.npy\", data_x)\n",
    "            \n",
    "            tf.variables_initializer([self.z_need_tune]).run()\n",
    "            for i in range(0,self.impute_iter):\n",
    "                _, impute_out, summary_str, impute_loss, imputed = self.sess.run([self.impute_optim, self.impute_out, self.impute_sum, self.impute_loss, self.imputed], \\\n",
    "                                                       feed_dict={self.x: data_x,\n",
    "                                                                  self.m: data_m,\n",
    "                                                                  self.deltaPre: data_deltaPre,\n",
    "                                                                  self.mean: data_mean,\n",
    "                                                                  self.x_lengths: data_x_lengths,\n",
    "                                                                  self.lastvalues: data_lastvalues,\n",
    "                                                                  self.deltaSub:deltaSub,\n",
    "                                                                  self.subvalues:subvalues,\n",
    "                                                                  self.imputed_m:imputed_m,\n",
    "                                                                  self.imputed_deltapre:imputed_deltapre,\n",
    "                                                                  self.imputed_deltasub:imputed_deltasub,\n",
    "                                                                  self.keep_prob: 1.0})\n",
    "                impute_tune_time+=1\n",
    "                counter+=1\n",
    "                if counter%10==0:\n",
    "                    print(\"Batchid: [%2d] [%4d/%4d] time: %4.4f, impute_loss: %.8f\" \\\n",
    "                          % (batchid, impute_tune_time, self.impute_iter, time.time() - start_time, impute_loss))\n",
    "                    self.writer.add_summary(summary_str, counter/10)\n",
    "            #imputed=tf.multiply((1-self.m),impute_out)+data_x\n",
    "            self.save_imputation(imputed,batchid,data_x_lengths,data_deltaPre,data_y,isTrain)\n",
    "            batchid+=1\n",
    "            impute_tune_time=1\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(\n",
    "            self.epoch,self.disc_iters,\n",
    "            self.batch_size, self.z_dim,\n",
    "            self.lr,self.impute_iter,\n",
    "            self.isNormal,self.isbatch_normal,\n",
    "            self.isSlicing,self.g_loss_lambda,\n",
    "            self.beta1\n",
    "            )\n",
    "\n",
    "\n",
    "    def save_imputation(self,impute_out,batchid,data_x_lengths,data_times,data_y,isTrain):\n",
    "       \n",
    "        if isTrain:\n",
    "            imputation_dir=\"588/imputation_train_results\"\n",
    "        else:\n",
    "            imputation_dir=\"imputation_test_results\"\n",
    "        \n",
    "        if not os.path.exists(os.path.join(imputation_dir,\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir)):\n",
    "            os.makedirs(os.path.join(imputation_dir,\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir))\n",
    "            \n",
    "        #write imputed data\n",
    "        resultFile=open(os.path.join(imputation_dir,\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir,\\\n",
    "                                     \"batch\"+str(batchid)+\"x\"),'w')\n",
    "        for length in data_x_lengths:\n",
    "            resultFile.writelines(str(length)+\",\")\n",
    "        resultFile.writelines(\"\\r\\n\")\n",
    "        # impute_out:ndarray\n",
    "        for oneSeries in impute_out:\n",
    "            resultFile.writelines(\"begin\\r\\n\")\n",
    "            for oneClass in oneSeries:\n",
    "                for i in oneClass.flat:\n",
    "                    resultFile.writelines(str(i)+\",\")\n",
    "                resultFile.writelines(\"\\r\\n\")\n",
    "            resultFile.writelines(\"end\\r\\n\")\n",
    "        resultFile.close()\n",
    "        \n",
    "        #write data_times data_times:list\n",
    "        resultFile=open(os.path.join(imputation_dir,\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir,\\\n",
    "                                     \"batch\"+str(batchid)+\"delta\"),'w')\n",
    "        for oneSeries in data_times:\n",
    "            resultFile.writelines(\"begin\\r\\n\")\n",
    "            for oneClass in oneSeries:\n",
    "                for i in oneClass:\n",
    "                    resultFile.writelines(str(i)+\",\")\n",
    "                resultFile.writelines(\"\\r\\n\")\n",
    "            resultFile.writelines(\"end\\r\\n\")\n",
    "        resultFile.close()\n",
    "        \n",
    "        \n",
    "    def writeG_Samples(self,filename,step,o):\n",
    "        if not os.path.exists(os.path.join(\"G_results\",\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir)):\n",
    "            os.makedirs(os.path.join(\"G_results\",\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir))\n",
    "        resultFile=open(os.path.join(\"G_results\",\\\n",
    "                                     self.model_name,\\\n",
    "                                     self.model_dir,\\\n",
    "                                     filename+str(step)),'w')\n",
    "        for oneSeries in o:\n",
    "            resultFile.writelines(\"begin\\r\\n\")\n",
    "            for oneClass in oneSeries:\n",
    "                for i in oneClass.flat:\n",
    "                    resultFile.writelines(str(i)+\",\")\n",
    "                resultFile.writelines(\"\\r\\n\")\n",
    "            resultFile.writelines(\"end\\r\\n\")\n",
    "        resultFile.close()\n",
    "    \n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name, self.model_dir )\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_name, self.model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b82010-b9a2-4232-8893-ae7eaeebb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[205815.0]\n",
      "[1720]\n",
      "22\n",
      "max_length is : 344\n",
      "non_in_dic_count is : 0\n",
      "344.0\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "class ReadPhysionetData():\n",
    "    # first read all dataset\n",
    "    # before call, determine wheher shuffle\n",
    "    # produce next batch\n",
    "    def __init__(self, dataPath, isNormal):\n",
    "\n",
    "        fileNames=os.listdir('E:/master/thesis/code/mutivariate/data/588')[1:]\n",
    "        labels=[1,0]*5\n",
    "        \n",
    "        self.dataPath = dataPath\n",
    "        self.fileNames = fileNames\n",
    "\n",
    "        dic={'time':-1,'glucose_value':0}\n",
    "    \n",
    "        self.dic=dic\n",
    "        mean=[0.0]*(len(dic)-1)\n",
    "        meancount=[0]*(len(dic)-1)\n",
    "        x=[]\n",
    "        times=[]\n",
    "        non_in_dic_count=0\n",
    "        # times: totalFilesLength*steps\n",
    "        # x: totalFilesLength*steps*feature_length\n",
    "        for fileName in fileNames:\n",
    "            f=open(os.path.join(self.dataPath, fileName))\n",
    "            count=0\n",
    "            \n",
    "            lastTime=0\n",
    "            totalData=[]\n",
    "            t_times=[]\n",
    "            for line in f.readlines():\n",
    "                if count >= 0:\n",
    "                    words=line.strip().split(\",\")\n",
    "                    timestamp=words[0]\n",
    "                    feature='glucose_value'\n",
    "                    value=words[1]\n",
    "                \n",
    "                    if timestamp!=lastTime:\n",
    "                        data=[0.0]*(len(dic)-1)\n",
    "                        \n",
    "                        t_times.append(float((datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S') - datetime.strptime('2021-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')).total_seconds())/60)\n",
    "\n",
    "                        data[self.dic[feature]]=float(value)\n",
    "                        mean[self.dic[feature]]+=float(value)\n",
    "                        meancount[self.dic[feature]]+=1\n",
    "\n",
    "                        totalData.append(data)\n",
    "                    else:\n",
    "                        \n",
    "                        totalData[-1][self.dic[feature]]=float(value)\n",
    "                        mean[self.dic[feature]]+=float(value)\n",
    "                        meancount[self.dic[feature]]+=1\n",
    "                            \n",
    "                            \n",
    "                    lastTime=timestamp      \n",
    "                count+=1\n",
    "                \n",
    "            \n",
    "            x.append(totalData)\n",
    "            times.append(t_times)\n",
    "            f.close()\n",
    "       \n",
    "        self.x=x\n",
    "        self.y=labels\n",
    "        self.times=times\n",
    "       \n",
    "        \n",
    "        print(mean)\n",
    "        print(meancount)\n",
    "        for i in range(len(mean)):\n",
    "            if meancount[i]!=0:\n",
    "                mean[i]=mean[i]/meancount[i]\n",
    "        self.mean=mean\n",
    "        \n",
    "        \n",
    "        # normalization\n",
    "        m=[] # mask 0/1\n",
    "        # first calculate std\n",
    "        self.std=[0.0]*(len(dic)-1)\n",
    "        for onefile in self.x:\n",
    "            one_m=[]\n",
    "            for oneclass in onefile:\n",
    "                t_m=[0]*len(oneclass)\n",
    "                for j in range(len(oneclass)):\n",
    "                    if oneclass[j] !=0:\n",
    "                        self.std[j]+=(oneclass[j]-self.mean[j])**2\n",
    "                        t_m[j]=1\n",
    "                one_m.append(t_m)\n",
    "            m.append(one_m)\n",
    "        for j in range(len(self.std)):\n",
    "            self.std[j]=math.sqrt(1.0/(meancount[j]-1)*self.std[j])\n",
    "        \n",
    "        self.isNormal=isNormal\n",
    "        self.normalization(isNormal)    \n",
    "            \n",
    "                        \n",
    "        x_lengths=[]\n",
    "        deltaPre=[] #time difference \n",
    "        lastvalues=[] # if missing, last values, last observed value of each point\n",
    "        deltaSub=[]\n",
    "        subvalues=[]\n",
    "        for h in range(len(self.x)):\n",
    "            # oneFile: steps*value_number\n",
    "            oneFile=self.x[h]\n",
    "            one_time=self.times[h]\n",
    "            x_lengths.append(len(oneFile))\n",
    "            \n",
    "            one_deltaPre=[]\n",
    "            one_lastvalues=[]\n",
    "            \n",
    "            one_deltaSub=[]\n",
    "            one_subvalues=[]\n",
    "            \n",
    "            one_m=m[h]\n",
    "            for i in range(len(oneFile)):\n",
    "                t_deltaPre=[0.0]*len(oneFile[i])\n",
    "                t_lastvalue=[0.0]*len(oneFile[i])\n",
    "                one_deltaPre.append(t_deltaPre)\n",
    "                one_lastvalues.append(t_lastvalue)\n",
    "                \n",
    "                if i==0:\n",
    "                    for j in range(len(oneFile[i])):\n",
    "                        one_lastvalues[i][j]=0.0 if one_m[i][j]==0 else oneFile[i][j]\n",
    "                    continue\n",
    "                #i!=0\n",
    "                for j in range(len(oneFile[i])):\n",
    "                    if one_m[i-1][j]==1:\n",
    "                        one_deltaPre[i][j]=one_time[i]-one_time[i-1]\n",
    "                    if one_m[i-1][j]==0:\n",
    "                        one_deltaPre[i][j]=one_time[i]-one_time[i-1]+one_deltaPre[i-1][j]\n",
    "                        \n",
    "                    if one_m[i][j]==1:\n",
    "                        one_lastvalues[i][j]=oneFile[i][j]\n",
    "                    if one_m[i][j]==0:\n",
    "                        one_lastvalues[i][j]=one_lastvalues[i-1][j]\n",
    "        \n",
    "            for i in range(len(oneFile)):\n",
    "                t_deltaSub=[0.0]*len(oneFile[i])\n",
    "                t_subvalue=[0.0]*len(oneFile[i])\n",
    "                one_deltaSub.append(t_deltaSub)\n",
    "                one_subvalues.append(t_subvalue)\n",
    "            #construct array \n",
    "            for i in range(len(oneFile)-1,-1,-1):    \n",
    "                if i==len(oneFile)-1:\n",
    "                    for j in range(len(oneFile[i])):\n",
    "                        one_subvalues[i][j]=0.0 if one_m[i][j]==0 else oneFile[i][j]\n",
    "                    continue\n",
    "                for j in range(len(oneFile[i])):\n",
    "                    if one_m[i+1][j]==1:\n",
    "                        one_deltaSub[i][j]=one_time[i+1]-one_time[i]\n",
    "                    if one_m[i+1][j]==0:\n",
    "                        one_deltaSub[i][j]=one_time[i+1]-one_time[i]+one_deltaSub[i+1][j]\n",
    "                        \n",
    "                    if one_m[i][j]==1:\n",
    "                        one_subvalues[i][j]=oneFile[i][j]\n",
    "                    if one_m[i][j]==0:\n",
    "                        one_subvalues[i][j]=one_subvalues[i+1][j]   \n",
    "                \n",
    "            \n",
    "            #m.append(one_m)\n",
    "            deltaPre.append(one_deltaPre)\n",
    "            lastvalues.append(one_lastvalues)\n",
    "            deltaSub.append(one_deltaSub)\n",
    "            subvalues.append(one_subvalues)\n",
    "        self.m=m\n",
    "        self.deltaPre=deltaPre\n",
    "        self.lastvalues=lastvalues\n",
    "        self.deltaSub=deltaSub\n",
    "        self.subvalues=subvalues\n",
    "        self.x_lengths=x_lengths\n",
    "        self.maxLength=max(x_lengths)\n",
    "        \n",
    "        print(\"max_length is : \"+str(self.maxLength))\n",
    "        print(\"non_in_dic_count is : \"+str(non_in_dic_count))\n",
    "        \n",
    "        resultFile=open(os.path.join(\"588/\",\"meanAndstd\"),'w')\n",
    "        for i in range(len(self.mean)):\n",
    "            resultFile.writelines(str(self.mean[i])+\",\"+str(self.std[i])+\",\"+str(meancount[i])+\"\\r\\n\")\n",
    "        resultFile.close()\n",
    "        \n",
    "    def normalization(self,isNormal):\n",
    "        if not isNormal:\n",
    "            return\n",
    "        for onefile in self.x:\n",
    "            for oneclass in onefile:\n",
    "                for j in range(len(oneclass)):\n",
    "                    if oneclass[j] !=0:\n",
    "                        if self.std[j]==0:\n",
    "                            oneclass[j]=0.0\n",
    "                        else:\n",
    "                            oneclass[j]=1.0/self.std[j]*(oneclass[j]-self.mean[j])\n",
    "                            \n",
    "        print('22')\n",
    "       \n",
    "    \n",
    "    def nextBatch(self):\n",
    "        print('33')\n",
    "        i=1\n",
    "        #batchsize: 128, len(x)=3594\n",
    "        while i*self.batchSize<=len(self.x):\n",
    "            #sub_time=[]\n",
    "            x=[]\n",
    "            y=[]\n",
    "            m=[]\n",
    "            deltaPre=[]\n",
    "            x_lengths=[]\n",
    "            lastvalues=[]\n",
    "            deltaSub=[]\n",
    "            subvalues=[]\n",
    "            imputed_deltapre=[]\n",
    "            imputed_m=[]\n",
    "            imputed_deltasub=[]\n",
    "            mean=self.mean\n",
    "            files=[]\n",
    "            \n",
    "            # 0-128 / 128-256...\n",
    "            for j in range((i-1)*self.batchSize,i*self.batchSize):\n",
    "                files.append(self.fileNames[j])\n",
    "                \n",
    "                #sub_time.append(self.times[j])\n",
    "                x.append(self.x[j])\n",
    "                y.append(self.y[j])\n",
    "                m.append(self.m[j])\n",
    "                deltaPre.append(self.deltaPre[j])\n",
    "                deltaSub.append(self.deltaSub[j])\n",
    "               \n",
    "                x_lengths.append(self.x_lengths[j])\n",
    "                lastvalues.append(self.lastvalues[j])\n",
    "                subvalues.append(self.subvalues[j])\n",
    "                \n",
    "                #0-128, i=1 / 128-256, i=2 ... -> jj: 0-128\n",
    "                jj=j-(i-1)*self.batchSize\n",
    "                #times.append(self.times[j])\n",
    "                while len(x[jj])<self.maxLength:\n",
    "                   \n",
    "                    t1=[0.0]*(len(self.dic)-1)\n",
    "                    x[jj].append(t1)\n",
    "                    #times[jj].append(0.0)\n",
    "                    t2=[0]*(len(self.dic)-1)\n",
    "                    m[jj].append(t2)\n",
    "                    t3=[0.0]*(len(self.dic)-1)\n",
    "                    deltaPre[jj].append(t3)\n",
    "                    t4=[0.0]*(len(self.dic)-1)\n",
    "                    lastvalues[jj].append(t4)\n",
    "                    t5=[0.0]*(len(self.dic)-1)\n",
    "                    deltaSub[jj].append(t5)\n",
    "                    t6=[0.0]*(len(self.dic)-1)\n",
    "                    subvalues[jj].append(t6)\n",
    "           \n",
    "            for j in range((i-1)*self.batchSize,i*self.batchSize):\n",
    "                one_imputed_deltapre=[]\n",
    "                one_imputed_deltasub=[]\n",
    "                one_G_m=[]\n",
    "               \n",
    "                for h in range(0,self.x_lengths[j]):\n",
    "                        \n",
    "                    if h==0:\n",
    "                        one_f_time=[0.0]*(len(self.dic)-1)\n",
    "                        one_imputed_deltapre.append(one_f_time)\n",
    "                        try:\n",
    "                            one_sub=[self.times[j][h+1]-self.times[j][h]]*(len(self.dic)-1)\n",
    "                        except:\n",
    "                            print(\"error: \"+str(h)+\" \"+str(len(self.times[j]))+\" \"+self.fileNames[j])\n",
    "                        one_imputed_deltasub.append(one_sub)\n",
    "                        one_f_g_m=[1.0]*(len(self.dic)-1)\n",
    "                        one_G_m.append(one_f_g_m)\n",
    "                    elif h==self.x_lengths[j]-1:\n",
    "                        one_f_time=[self.times[j][h]-self.times[j][h-1]]*(len(self.dic)-1)\n",
    "                        one_imputed_deltapre.append(one_f_time)\n",
    "                        one_sub=[0.0]*(len(self.dic)-1)\n",
    "                        one_imputed_deltasub.append(one_sub)\n",
    "                        one_f_g_m=[1.0]*(len(self.dic)-1)\n",
    "                        one_G_m.append(one_f_g_m)\n",
    "                    else:\n",
    "                        one_f_time=[self.times[j][h]-self.times[j][h-1]]*(len(self.dic)-1)\n",
    "                        one_imputed_deltapre.append(one_f_time)\n",
    "                        one_sub=[self.times[j][h+1]-self.times[j][h]]*(len(self.dic)-1)\n",
    "                        one_imputed_deltasub.append(one_sub)\n",
    "                        one_f_g_m=[1.0]*(len(self.dic)-1)\n",
    "                        one_G_m.append(one_f_g_m)\n",
    "                while len(one_imputed_deltapre)<self.maxLength:\n",
    "                    one_f_time=[0.0]*(len(self.dic)-1)\n",
    "                    one_imputed_deltapre.append(one_f_time)\n",
    "                    one_sub=[0.0]*(len(self.dic)-1)\n",
    "                    one_imputed_deltasub.append(one_sub)\n",
    "                    one_f_g_m=[0.0]*(len(self.dic)-1)\n",
    "                    one_G_m.append(one_f_g_m)\n",
    "                imputed_deltapre.append(one_imputed_deltapre)\n",
    "                imputed_deltasub.append(one_imputed_deltasub)\n",
    "                imputed_m.append(one_G_m)\n",
    "               \n",
    "            i+=1\n",
    "            if self.isNormal:\n",
    "                yield  x,y,[0.0]*(len(self.dic)-1),m,deltaPre,x_lengths,lastvalues,files,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub\n",
    "            else:\n",
    "                yield x,y,mean,m,deltaPre,x_lengths,lastvalues,files,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub\n",
    "            \n",
    "                \n",
    "        \n",
    "    def shuffle(self,batchSize=10,isShuffle=False):\n",
    "        self.batchSize=batchSize\n",
    "        if isShuffle:\n",
    "            c = list(zip(self.x,self.y,self.m,self.deltaPre,self.x_lengths,self.lastvalues,self.fileNames,self.times,self.deltaSub,self.subvalues))\n",
    "            random.shuffle(c)\n",
    "            self.x,self.y,self.m,self.deltaPre,self.x_lengths,self.lastvalues,self.fileNames,self.times,self.deltaSub,self.subvalues=zip(*c)\n",
    "        #print('ww')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    dt=ReadPhysionetData(\"data/588/\",isNormal=True)\n",
    "    dt.shuffle(5,False)\n",
    "    batchCount=1\n",
    "    X_lengths=dt.x_lengths\n",
    "    Time=dt.times[-144:-16]\n",
    "    print(sum(X_lengths)/len(X_lengths))\n",
    "    \n",
    "    for x,y,mean,m,deltaPre,x_lengths,lastvalues,files,imputed_deltapre,imputed_m,deltaSub,subvalues,imputed_deltasub in dt.nextBatch():\n",
    "        #print(x)\n",
    "        batchCount+=1\n",
    "        if batchCount%100==0:\n",
    "            print(files)\n",
    "\n",
    "def f():\n",
    "    print(\"readData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d418dc7-b67d-4344-9e7e-86778f86226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"main\"\"\"\n",
    "def main():\n",
    "    # parse arguments\n",
    "    \n",
    "    args = argparse.Namespace()\n",
    "    args.gpus = None\n",
    "    args.batch_size = 5\n",
    "    args.gen_length = 96\n",
    "    args.impute_iter = 400\n",
    "    args.pretrain_epoch = 5\n",
    "    args.run_type = 'train'\n",
    "    args.data_path = \"data/588/\"\n",
    "    args.model_path = None\n",
    "    args.result_path = None\n",
    "    args.dataset_name = None\n",
    "    args.g_loss_lambda = 0.15\n",
    "    args.beta1 = 0.5\n",
    "    args.lr = 0.0005\n",
    "    args.epoch = 15\n",
    "    args.n_inputs = 1\n",
    "    args.n_hidden_units = 64\n",
    "    args.n_classes = 2\n",
    "    args.z_dim = 256\n",
    "    args.checkpoint_dir = '588/checkpoint'\n",
    "    args.result_dir = '588/results'\n",
    "    args.log_dir = '588/logs'\n",
    "    args.isNormal = 1\n",
    "    args.isBatch_normal = 1\n",
    "    args.isSlicing = 1\n",
    "    args.disc_iters = 8\n",
    "    \n",
    "    if args.isBatch_normal==0:\n",
    "            args.isBatch_normal=False\n",
    "    if args.isBatch_normal==1:\n",
    "            args.isBatch_normal=True\n",
    "    if args.isNormal==0:\n",
    "            args.isNormal=False\n",
    "    if args.isNormal==1:\n",
    "            args.isNormal=True\n",
    "    if args.isSlicing==0:\n",
    "            args.isSlicing=False\n",
    "    if args.isSlicing==1:\n",
    "            args.isSlicing=True\n",
    "\n",
    "    #make the max step length of two datasett the same\n",
    "    epochs=[15]\n",
    "    g_loss_lambdas=[0.15]\n",
    "    beta1s=[0.5]\n",
    "    for beta1 in beta1s:\n",
    "        for e in epochs:\n",
    "            for g_l in g_loss_lambdas:\n",
    "                args.epoch=e\n",
    "                args.beta1=beta1\n",
    "                args.g_loss_lambda=g_l\n",
    "                tf.reset_default_graph()\n",
    "                dt_train=ReadPhysionetData(os.path.join(args.data_path),isNormal=args.isNormal)\n",
    "               \n",
    "                tf.reset_default_graph()\n",
    "                config = tf.ConfigProto() \n",
    "                config.gpu_options.allow_growth = True \n",
    "                with tf.Session(config=config) as sess:\n",
    "                    gan = WGAN(sess,\n",
    "                                args=args,\n",
    "                                datasets=dt_train,\n",
    "                                )\n",
    "            \n",
    "                    # build graph\n",
    "                    gan.build_model()\n",
    "            \n",
    "                   \n",
    "                    gan.train()\n",
    "                    print(\" [*] Training finished!\")\n",
    "                    \n",
    "                    gan.imputation(dt_train,True)\n",
    "                    \n",
    "                    print(\" [*] Train dataset Imputation finished!\")\n",
    "                    \n",
    "                    \n",
    "                    print(\" [*] Test dataset Imputation finished!\")\n",
    "                tf.reset_default_graph()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d019b3-445c-491a-8c5e-4e89d220f8f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a631ccb9-c6fa-425a-9ebe-008dd6fdb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original missing data\n",
    "def read_ori(filename):\n",
    "    values = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  \n",
    "            parts = line.split(',') \n",
    "            if len(parts) >= 2: \n",
    "                value_str = parts[1].strip() \n",
    "                try:\n",
    "                    value = float(value_str) \n",
    "                    values.append(value) \n",
    "                except ValueError:\n",
    "                    pass  \n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6757bfba-81ba-44a9-88e0-444ff4fee0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read imputed data\n",
    "def read_text(filename):\n",
    "    glucose = []\n",
    "    with open(filename, 'r') as file:\n",
    "      \n",
    "        next(file)\n",
    "        \n",
    "      \n",
    "        for line in file:\n",
    "            line = line.strip() \n",
    "            if line == \"begin\":\n",
    "                glucose.append([])  \n",
    "            elif line == \"end\":\n",
    "                pass  \n",
    "            elif line:  \n",
    "                value = float(line.strip(',')) \n",
    "                glucose[-1].append(value)  \n",
    "    \n",
    "    return glucose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4150e9-3f32-4a42-96c4-8241e79836f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (data, parameters=None):\n",
    "  '''Normalize data in [0, 1] range.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "  \n",
    "  Returns:\n",
    "    - norm_data: normalized data\n",
    "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
    "  '''\n",
    "\n",
    "  # Parameters\n",
    "  _, dim = data.shape\n",
    "  norm_data = data.copy()\n",
    "  \n",
    "  if parameters is None:\n",
    "  \n",
    "    # MixMax normalization\n",
    "    min_val = np.nanmin(norm_data)\n",
    "    norm_data = norm_data - np.nanmin(norm_data)\n",
    "    max_val = np.nanmax(norm_data)\n",
    "    norm_data = norm_data / (np.nanmax(norm_data) + 1e-6)  \n",
    "    \n",
    "    norm_parameters = {'min_val': min_val,\n",
    "                       'max_val': max_val}\n",
    "    \n",
    "  else:\n",
    "    min_val = parameters['min_val']\n",
    "    max_val = parameters['max_val']\n",
    "\n",
    "    # For each dimension\n",
    "    norm_data = norm_data - min_val\n",
    "    norm_data = norm_data / (max_val + 1e-6)  \n",
    "\n",
    "    norm_parameters = parameters  \n",
    "          \n",
    "      \n",
    "  return norm_data, norm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ca0cf9-2e33-48ca-84dd-567b6df533bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss (ori_data, imputed_data, data_m):\n",
    "  '''Compute RMSE loss between ori_data and imputed_data\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data without missing values\n",
    "    - imputed_data: imputed data\n",
    "    - data_m: indicator matrix for missingness\n",
    "    \n",
    "  Returns:\n",
    "    - rmse: Root Mean Squared Error\n",
    "  '''\n",
    "  \n",
    "  ori_data, norm_parameters = normalization(ori_data)\n",
    "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
    "    \n",
    "  # Only for missing values\n",
    "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
    "  denominator = np.sum(1-data_m)\n",
    "  \n",
    "  rmse = np.sqrt(nominator/float(denominator))\n",
    "  \n",
    "  return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775069a8-95df-4b12-944d-fb27f93c1c5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient 570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58c18f6e-178d-4fd0-8f8d-ac467772de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"data/570/5701.txt\"\n",
    "filename2 = \"data/570/5702.txt\"\n",
    "filename3 = \"data/570/5703.txt\"\n",
    "filename4 = \"data/570/5704.txt\"\n",
    "filename5 = \"data/570/5705.txt\"\n",
    "filename6 = \"data/570/5706.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)\n",
    "value6 = read_ori(filename6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1a18f29-1f70-4a4b-b421-5d5b653995fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_570 = value1 + value2 + value3 + value4 + value5 + value6 \n",
    "# mask list\n",
    "mask_list_570 = [1 if value != 0 else 0 for value in glucose_miss_ori_570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49a2da57-c2af-4714-a89a-411dad5c7962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data without missing values\n",
    "glucose_nomiss_ori_570 = pd.read_csv('data/570_12.8_1614.csv')['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af3fcd9a-2fa6-4987-a7a2-a0d06b211fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_570 = 143.54089219330854\n",
    "std_570 = 65.08417318014587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "216f70fd-e018-4663-bb7e-22b221983dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read normalized imputed values\n",
    "filename_570 = \"570/imputation_train_results/WGAN_no_mask/45_8_6_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_570 = read_text(filename_570)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_570 = np.array(glucose_imputed_norm_570)*std_570+mean_570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22d018dc-5de5-4871-8511-2c37d40ab566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_570_final = glucose_imputed_570[5].reshape(269,).tolist() + glucose_imputed_570[3].reshape(269,).tolist() + glucose_imputed_570[2].reshape(269,).tolist() + glucose_imputed_570[0].reshape(269,).tolist() + glucose_imputed_570[4].reshape(269,).tolist() + glucose_imputed_570[1].reshape(269,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c498f45f-b4ce-451c-bf25-3dea156b9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_570_final).to_csv('570/570_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcb5cb-4634-428b-b149-acdc5d7d7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_570).reshape(1614,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_570_final).reshape(1614,1), norm_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3926bea-a0e0-4913-b0d9-897389e9de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_570).reshape(1614,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a928568b-7c9f-4381-9681-b5139d22f0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3158442876852117"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33bad555-2a6b-491b-94ed-7a7d5fb8c0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2847668037678422"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_570_miss = pd.read_csv('data/570_miss.csv')\n",
    "df_570_miss['value'] = df_570_miss['value'].fillna(143.54089219330854)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_570).reshape(1614,1))\n",
    "imputed_data, _ = normalization(np.array(df_570_miss['value']).reshape(1614,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_570).reshape(1614,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefde206-ccbb-42ba-84a8-f625db5adfd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient 559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "042bf4e4-8024-4a1c-8dd3-4a47077ad179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original data \n",
    "filename1 = \"data/559/5591.txt\"\n",
    "filename2 = \"data/559/5592.txt\"\n",
    "filename3 = \"data/559/5593.txt\"\n",
    "filename4 = \"data/559/5594.txt\"\n",
    "filename5 = \"data/559/5595.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6d1b514-3d51-43ed-8bc9-f1ea670f8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_559 = value1 + value2 + value3 + value4 + value5\n",
    "# mask list\n",
    "mask_list_559 = [1 if value != 0 else 0 for value in glucose_miss_ori_559]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "655742fc-ba6e-4a77-a16a-b2e59fe4f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data without missing values\n",
    "glucose_nomiss_ori_559 = pd.read_csv('data/559_12.27_1290.csv')['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d710de57-cb1e-4a4a-b01d-641509bd091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_559 = 127.24573643410852\n",
    "std_559 = 61.36112157884535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e999ead-2989-4feb-a68e-c4b7d765db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the imputed data\n",
    "filename_559 = \"559/imputation_train_results/WGAN_no_mask/40_8_5_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_559 = read_text(filename_559)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_559 = np.array(glucose_imputed_norm_559)*std_559+mean_559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27350a07-0db3-455d-85e5-0f5231281966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1290"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glucose_imputed_559_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7110237e-62e4-41af-b98a-173cfce71259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_559_final = glucose_imputed_559[0].reshape(258,).tolist() + glucose_imputed_559[1].reshape(258,).tolist() + glucose_imputed_559[2].reshape(258,).tolist() + glucose_imputed_559[4].reshape(258,).tolist() + glucose_imputed_559[3].reshape(258,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79dda3fc-0317-4126-a1fa-1eefc7109ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_559_final).to_csv('559/559_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "987e74a7-698e-4355-8da3-05e9d37f3e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24311996810677758"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_559).reshape(1290,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_559_final).reshape(1290,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_559).reshape(1290,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3902a28c-1b39-410d-8e53-2bd7fd529a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28791818922349305"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_559_miss = pd.read_csv('data/559_miss.csv')\n",
    "df_559_miss['value'] = df_559_miss['value'].fillna(127.24573643410852)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_559).reshape(1290,1))\n",
    "imputed_data, _ = normalization(np.array(df_559_miss['value']).reshape(1290,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_559).reshape(1290,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8df7c-1e50-45c7-9905-d9a399a13dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient 563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c17a00e-4cc0-4671-829b-d67281123312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original data \n",
    "filename1 = \"data/563/5631.txt\"\n",
    "filename2 = \"data/563/5632.txt\"\n",
    "filename3 = \"data/563/5633.txt\"\n",
    "filename4 = \"data/563/5634.txt\"\n",
    "filename5 = \"data/563/5635.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3255a599-aaab-4f82-acd6-9aff37c6bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_563 = value1 + value2 + value3 + value4 + value5\n",
    "# mask list\n",
    "mask_list_563 = [1 if value != 0 else 0 for value in glucose_miss_ori_563]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55048d8d-b036-49df-a739-62cef498851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data without missing values\n",
    "glucose_nomiss_ori_563 = pd.read_csv('data/563_10.15_1726.csv')['value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e458e30-de1d-4ca8-9d15-e7e8a5bc039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_563 = 119.84869565217392\n",
    "std_563 = 51.592521406907515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "498a9fdf-d0bd-4188-a0d0-f3127e09dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the imputed data\n",
    "filename_563 = \"563/imputation_train_results/WGAN_no_mask/30_8_5_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_563 = read_text(filename_563)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_563 = np.array(glucose_imputed_norm_563)*std_563+mean_563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22cf67c6-0a91-4de3-96a9-1a875480835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_563_final = glucose_imputed_563[3].reshape(345,).tolist() + glucose_imputed_563[0].reshape(345,).tolist() + glucose_imputed_563[2].reshape(345,).tolist() + glucose_imputed_563[4].reshape(345,).tolist() + glucose_imputed_563[1].reshape(345,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a0a4d2-6523-409b-9410-2c200a1816ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_563_final).to_csv('563/563_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "242db206-4e60-44de-b708-f7f2c5894b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2980753930515585"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_563).reshape(1725,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_563_final).reshape(1725,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_563).reshape(1725,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c512e6b2-1ea8-49d8-bebb-5d8c1668d7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2879509508751353"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_563_miss = pd.read_csv('data/563_miss.csv')\n",
    "df_563_miss['value'] = df_563_miss['value'].fillna(119.84869565217392)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_563).reshape(1725,1))\n",
    "imputed_data, _ = normalization(np.array(df_563_miss['value']).reshape(1725,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_563).reshape(1725,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0cb7e0-e261-47d3-9356-afebc4131edc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient 575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5efa131-dd43-4c24-b8ea-741ccdab97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original data \n",
    "filename1 = \"data/575/5751.txt\"\n",
    "filename2 = \"data/575/5752.txt\"\n",
    "filename3 = \"data/575/5753.txt\"\n",
    "filename4 = \"data/575/5754.txt\"\n",
    "filename5 = \"data/575/5755.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d10565ae-d0f1-40c9-a450-a54ceb00922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_575 = value1 + value2 + value3 + value4 + value5\n",
    "# mask list\n",
    "mask_list_575 = [1 if value != 0 else 0 for value in glucose_miss_ori_575]\n",
    "# original data without missing values\n",
    "glucose_nomiss_ori_575 = pd.read_csv('data/575_1.1_731.csv')['value'].tolist()\n",
    "\n",
    "mean_575 = 125.80684931506849\n",
    "std_575 = 73.12277605521172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cbb551b-80fd-43ce-a994-963aa6f9bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the imputed data\n",
    "filename_575 = \"575/imputation_train_results/WGAN_no_mask/30_8_5_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_575 = read_text(filename_575)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_575 = np.array(glucose_imputed_norm_575)*std_575+mean_575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb3be4ed-f8ac-4b1c-bce4-0c20eee0220d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 146)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glucose_imputed_575.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25ecb231-d56e-4c49-a34f-9a9eba6be141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_575_final = glucose_imputed_575[1].reshape(146,).tolist() + glucose_imputed_575[3].reshape(146,).tolist() + glucose_imputed_575[4].reshape(146,).tolist() + glucose_imputed_575[0].reshape(146,).tolist() + glucose_imputed_575[2].reshape(146,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79ef9112-e2f4-4e59-aa2a-99063e543c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_575_final).to_csv('575/575_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "716e0917-d345-4293-9b91-d029baf7b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23505804566877062"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_575).reshape(730,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_575_final).reshape(730,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_575).reshape(730,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd263533-bc5e-4691-8e69-bfb399cf8881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23939629001445042"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_575_miss = pd.read_csv('data/575_miss.csv')\n",
    "df_575_miss['value'] = df_575_miss['value'].fillna(125.80684931506849)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_575).reshape(730,1))\n",
    "imputed_data, _ = normalization(np.array(df_575_miss['value']).reshape(730,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_575).reshape(730,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3828a-f11e-4ef2-a55a-837d3dcae357",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Patient 588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fae3c4aa-a285-4c6a-891d-ad076303aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original data \n",
    "filename1 = \"data/588/5881.txt\"\n",
    "filename2 = \"data/588/5882.txt\"\n",
    "filename3 = \"data/588/5883.txt\"\n",
    "filename4 = \"data/588/5884.txt\"\n",
    "filename5 = \"data/588/5885.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ecde0a6-5ed9-4351-81e5-71f8622b07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_588 = value1 + value2 + value3 + value4 + value5\n",
    "# mask list\n",
    "mask_list_588 = [1 if value != 0 else 0 for value in glucose_miss_ori_588]\n",
    "# original data without missing values\n",
    "glucose_nomiss_ori_588 = pd.read_csv('data/588_9.17_1720.csv')['value'].tolist()\n",
    "\n",
    "mean_588 = 119.65988372093024\n",
    "std_588 = 44.89488026998585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d013b0-df8d-4125-a967-2df9cc4b7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the imputed data\n",
    "filename_588 = \"588/imputation_train_results/WGAN_no_mask/15_8_5_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_588 = read_text(filename_588)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_588 = np.array(glucose_imputed_norm_588)*std_588+mean_588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8e66f7-970a-4b4c-97f5-8aa4fca969bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glucose_imputed_588_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac3deba-f0aa-4bc2-a907-df0911672025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_588_final = glucose_imputed_588[2].reshape(344,).tolist() + glucose_imputed_588[1].reshape(344,).tolist() + glucose_imputed_588[0].reshape(344,).tolist() + glucose_imputed_588[3].reshape(344,).tolist() + glucose_imputed_588[4].reshape(344,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4feb0ffe-af08-4486-8deb-9ccbd05d1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_588_final).to_csv('588/588_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de66012b-c6af-4c6c-aab3-61f82166e5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23615638925618718"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_588).reshape(1720,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_588_final).reshape(1720,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_588).reshape(1720,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67a56e9d-506c-461c-9c7f-fb79ad9fb732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23905359198335752"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_588_miss = pd.read_csv('data/588_miss.csv')\n",
    "df_588_miss['value'] = df_588_miss['value'].fillna(119.65988372093024)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_588).reshape(1720,1))\n",
    "imputed_data, _ = normalization(np.array(df_588_miss['value']).reshape(1720,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_588).reshape(1720,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfb6d6-1f71-4b1a-9b60-e20dd363918c",
   "metadata": {},
   "source": [
    "# Patient 591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a975db56-a19e-4488-838e-fda4033aba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read original data \n",
    "filename1 = \"data/591/5911.txt\"\n",
    "filename2 = \"data/591/5912.txt\"\n",
    "filename3 = \"data/591/5913.txt\"\n",
    "filename4 = \"data/591/5914.txt\"\n",
    "filename5 = \"data/591/5915.txt\"\n",
    "value1 = read_ori(filename1)\n",
    "value2 = read_ori(filename2)\n",
    "value3 = read_ori(filename3)\n",
    "value4 = read_ori(filename4)\n",
    "value5 = read_ori(filename5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a99a77de-87d0-42cb-b983-28f603a7466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orginal data with missing values in one list\n",
    "glucose_miss_ori_591 = value1 + value2 + value3 + value4 + value5\n",
    "# mask list\n",
    "mask_list_591 = [1 if value != 0 else 0 for value in glucose_miss_ori_591]\n",
    "# original data without missing values\n",
    "glucose_nomiss_ori_591 = pd.read_csv('data/591_1.10_1476.csv')['value'].tolist()\n",
    "\n",
    "mean_591 = 113.08338983050848\n",
    "std_591 = 48.667230310724904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3039419-42fe-47b7-86c7-a55b90ccb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the imputed data\n",
    "filename_591 = \"591/imputation_train_results/WGAN_no_mask/40_8_5_256_0.0005_400_True_True_True_0.15_0.5/batch1x\"\n",
    "glucose_imputed_norm_591 = read_text(filename_591)\n",
    "# imputed_norm -> orginal\n",
    "glucose_imputed_591 = np.array(glucose_imputed_norm_591)*std_591+mean_591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f4d45bf5-bebf-4700-b3b2-b2954948da70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1475"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glucose_imputed_591_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f97c4de-380f-4aff-944b-abc8d265d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine imputed data into one file, the same order with orginal data\n",
    "glucose_imputed_591_final = glucose_imputed_591[2].reshape(295,).tolist() + glucose_imputed_591[1].reshape(295,).tolist() + glucose_imputed_591[3].reshape(295,).tolist() + glucose_imputed_591[0].reshape(295,).tolist() + glucose_imputed_591[4].reshape(295,).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4618cf1c-0f73-4dd6-bf9a-777b04d89616",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(glucose_imputed_591_final).to_csv('591/591_imputed_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e646cd71-e1b1-4d39-af19-c8f50a948b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19407295757402662"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize orginal + imputed data\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_591).reshape(1475,1))\n",
    "imputed_data, _ = normalization(np.array(glucose_imputed_591_final).reshape(1475,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_591).reshape(1475,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdc01034-bf55-4449-96a6-19e62e8ef075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20663888455632368"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use mean value to impute data\n",
    "df_591_miss = pd.read_csv('data/591_miss.csv')\n",
    "df_591_miss['value'] = df_591_miss['value'].fillna(113.08338983050848)\n",
    "ori_data, norm_parameters = normalization(np.array(glucose_nomiss_ori_591).reshape(1475,1))\n",
    "imputed_data, _ = normalization(np.array(df_591_miss['value']).reshape(1475,1), norm_parameters)\n",
    "rmse = rmse_loss(ori_data, imputed_data, np.array(mask_list_591).reshape(1475,1))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362e3ef-886e-4707-907b-e3852d383c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
