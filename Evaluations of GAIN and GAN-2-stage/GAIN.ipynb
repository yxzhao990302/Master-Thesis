{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e3be4cf-5f21-463f-bc71-979336482676",
        "outputId": "fca3994d-180b-4511-9de6-2e9c4c0428c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# Necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm\n",
        "#import tensorflow as tf\n",
        "##IF USING TF 2 use following import to still use TF < 2.0 Functionalities\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "id": "3e3be4cf-5f21-463f-bc71-979336482676"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk-heplpgR4X",
        "outputId": "8abca965-626e-413e-8be2-d9241edaa8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "id": "uk-heplpgR4X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac531c85-c6c3-455f-a0da-06f3ec634277"
      },
      "outputs": [],
      "source": [
        "def normalization (data, parameters=None):\n",
        "  '''Normalize data in [0, 1] range.\n",
        "  \n",
        "  Args:\n",
        "    - data: original data\n",
        "  \n",
        "  Returns:\n",
        "    - norm_data: normalized data\n",
        "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
        "  '''\n",
        "\n",
        "  # Parameters\n",
        "  _, dim = data.shape\n",
        "  norm_data = data.copy()\n",
        "  \n",
        "  if parameters is None:\n",
        "  \n",
        "    # MixMax normalization\n",
        "    min_val = np.nanmin(norm_data)\n",
        "    norm_data = norm_data - np.nanmin(norm_data)\n",
        "    max_val = np.nanmax(norm_data)\n",
        "    norm_data = norm_data / (np.nanmax(norm_data) + 1e-6)  \n",
        "    \n",
        "    norm_parameters = {'min_val': min_val,\n",
        "                       'max_val': max_val}\n",
        "    \n",
        "  else:\n",
        "    min_val = parameters['min_val']\n",
        "    max_val = parameters['max_val']\n",
        "\n",
        "    # For each dimension\n",
        "    norm_data = norm_data - min_val\n",
        "    norm_data = norm_data / (max_val + 1e-6)  \n",
        "\n",
        "    norm_parameters = parameters  \n",
        "          \n",
        "      \n",
        "  return norm_data, norm_parameters"
      ],
      "id": "ac531c85-c6c3-455f-a0da-06f3ec634277"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eded6e21-4acf-482e-9231-de3055082ce6"
      },
      "outputs": [],
      "source": [
        "def renormalization (norm_data, norm_parameters):\n",
        "  '''Renormalize data from [0, 1] range to the original range.\n",
        "  \n",
        "  Args:\n",
        "    - norm_data: normalized data\n",
        "    - norm_parameters: min_val, max_val for each feature for renormalization\n",
        "  \n",
        "  Returns:\n",
        "    - renorm_data: renormalized original data\n",
        "  '''\n",
        "  \n",
        "  min_val = norm_parameters['min_val']\n",
        "  max_val = norm_parameters['max_val']\n",
        "\n",
        "  _, dim = norm_data.shape\n",
        "  renorm_data = norm_data.copy()\n",
        "    \n",
        "  renorm_data = renorm_data * (max_val + 1e-6)   \n",
        "  renorm_data = renorm_data + min_val\n",
        "    \n",
        "  return renorm_data"
      ],
      "id": "eded6e21-4acf-482e-9231-de3055082ce6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aa7324f-bb4c-4051-9929-d7ce47478838"
      },
      "outputs": [],
      "source": [
        "def rmse_loss (ori_data, imputed_data, data_m):\n",
        "  '''Compute RMSE loss between ori_data and imputed_data\n",
        "  \n",
        "  Args:\n",
        "    - ori_data: original data without missing values\n",
        "    - imputed_data: imputed data\n",
        "    - data_m: indicator matrix for missingness\n",
        "    \n",
        "  Returns:\n",
        "    - rmse: Root Mean Squared Error\n",
        "  '''\n",
        "  \n",
        "  ori_data, norm_parameters = normalization(ori_data)\n",
        "  imputed_data, _ = normalization(imputed_data, norm_parameters)\n",
        "    \n",
        "  # Only for missing values\n",
        "  nominator = np.sum(((1-data_m) * ori_data - (1-data_m) * imputed_data)**2)\n",
        "  denominator = np.sum(1-data_m)\n",
        "  \n",
        "  rmse = np.sqrt(nominator/float(denominator))\n",
        "  \n",
        "  return rmse"
      ],
      "id": "0aa7324f-bb4c-4051-9929-d7ce47478838"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bdd0f00-b749-447e-90bf-ee04601832f6"
      },
      "outputs": [],
      "source": [
        "def xavier_init(size):\n",
        "  '''Xavier initialization.\n",
        "  \n",
        "  Args:\n",
        "    - size: vector size\n",
        "    \n",
        "  Returns:\n",
        "    - initialized random vector.\n",
        "  '''\n",
        "  in_dim = size[0]\n",
        "  xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "  return tf.random_normal(shape = size, stddev = xavier_stddev)"
      ],
      "id": "9bdd0f00-b749-447e-90bf-ee04601832f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0860f208-6c0a-441d-99a1-4efefa386b6c"
      },
      "outputs": [],
      "source": [
        "def binary_sampler(p, rows, cols):\n",
        "  '''Sample binary random variables.\n",
        "  \n",
        "  Args:\n",
        "    - p: probability of 1\n",
        "    - rows: the number of rows\n",
        "    - cols: the number of columns\n",
        "    \n",
        "  Returns:\n",
        "    - binary_random_matrix: generated binary random matrix.\n",
        "  '''\n",
        "  unif_random_matrix = np.random.uniform(0., 1., size = [rows, cols])\n",
        "  binary_random_matrix = 1*(unif_random_matrix < p)\n",
        "  return binary_random_matrix"
      ],
      "id": "0860f208-6c0a-441d-99a1-4efefa386b6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d8f80c7-35a1-44ed-a12c-8edf2dbd7778"
      },
      "outputs": [],
      "source": [
        "\n",
        "def uniform_sampler(low, high, rows, cols):\n",
        "  '''Sample uniform random variables.\n",
        "  \n",
        "  Args:\n",
        "    - low: low limit\n",
        "    - high: high limit\n",
        "    - rows: the number of rows\n",
        "    - cols: the number of columns\n",
        "    \n",
        "  Returns:\n",
        "    - uniform_random_matrix: generated uniform random matrix.\n",
        "  '''\n",
        "  return np.random.uniform(low, high, size = [rows, cols])"
      ],
      "id": "4d8f80c7-35a1-44ed-a12c-8edf2dbd7778"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d122280b-fbf0-442f-acd0-c05eeaa3945f"
      },
      "outputs": [],
      "source": [
        "def sample_batch_index(total, batch_size):\n",
        "  '''Sample index of the mini-batch.\n",
        "  \n",
        "  Args:\n",
        "    - total: total number of samples\n",
        "    - batch_size: batch size\n",
        "    \n",
        "  Returns:\n",
        "    - batch_idx: batch index\n",
        "  '''\n",
        "  total_idx = np.random.permutation(total)\n",
        "  batch_idx = total_idx[:batch_size]\n",
        "  return batch_idx"
      ],
      "id": "d122280b-fbf0-442f-acd0-c05eeaa3945f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fdc737f-027c-4557-8791-1d097025ffab"
      },
      "outputs": [],
      "source": [
        "def data_loader (data_name, miss_rate):\n",
        "  '''Loads datasets and introduce missingness.\n",
        "  \n",
        "  Args:\n",
        "    - data_name: letter, spam, or mnist\n",
        "    - miss_rate: the probability of missing components\n",
        "    \n",
        "  Returns:\n",
        "    data_x: original data\n",
        "    miss_data_x: data with missing values\n",
        "    data_m: indicator matrix for missing components\n",
        "  '''\n",
        "  \n",
        "  # Load data\n",
        "  file_name = data_name + '.csv'\n",
        "  data_x = np.loadtxt(file_name, delimiter=\",\", skiprows=1)\n",
        " \n",
        "  # Parameters\n",
        "\n",
        "  data_x = data_x.reshape(1,1614)\n",
        "  no, dim = data_x.shape\n",
        "  \n",
        "  # Introduce missing data\n",
        "  data_m = binary_sampler(1-miss_rate, no, dim)\n",
        "  miss_data_x = data_x.copy()\n",
        "  miss_data_x[data_m == 0] = np.nan\n",
        "      \n",
        "  return data_x, miss_data_x, data_m"
      ],
      "id": "9fdc737f-027c-4557-8791-1d097025ffab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "479269a3-0847-4282-bd87-8b62fdd456ea"
      },
      "outputs": [],
      "source": [
        "def gain (data_x, gain_parameters):\n",
        "  '''Impute missing values in data_x\n",
        "  \n",
        "  Args:\n",
        "    - data_x: original data with missing values\n",
        "    - gain_parameters: GAIN network parameters:\n",
        "      - batch_size: Batch size\n",
        "      - hint_rate: Hint rate\n",
        "      - alpha: Hyperparameter\n",
        "      - iterations: Iterations\n",
        "      \n",
        "  Returns:\n",
        "    - imputed_data: imputed data\n",
        "  '''\n",
        "  # Define mask matrix\n",
        "  data_m = 1-np.isnan(data_x)\n",
        "  \n",
        "  # System parameters\n",
        "  batch_size = gain_parameters['batch_size']\n",
        "  hint_rate = gain_parameters['hint_rate']\n",
        "  alpha = gain_parameters['alpha']\n",
        "  iterations = gain_parameters['iterations']\n",
        "  \n",
        "  # Other parameters\n",
        "  no, dim = data_x.shape\n",
        "  \n",
        "  # Hidden state dimensions\n",
        "  h_dim = int(dim)\n",
        "  \n",
        "  # Normalization\n",
        "  norm_data, norm_parameters = normalization(data_x)\n",
        "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
        "  \n",
        "  ## GAIN architecture   \n",
        "  # Input placeholders\n",
        "  # Data vector\n",
        "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
        "  # Mask vector \n",
        "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
        "  # Hint vector\n",
        "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
        "  \n",
        "  # Discriminator variables\n",
        "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
        "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  D_W2 = tf.Variable(xavier_init([h_dim, int(h_dim/2)]))\n",
        "  D_b2 = tf.Variable(tf.zeros(shape = [int(h_dim/2)]))\n",
        "  \n",
        "  D_W3 = tf.Variable(xavier_init([int(h_dim/2), h_dim]))\n",
        "  D_b3 = tf.Variable(tf.zeros(shape = [h_dim]))  # Multi-variate outputs\n",
        "  \n",
        "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
        "  \n",
        "  #Generator variables\n",
        "  # Data + Mask as inputs (Random noise is in missing components)\n",
        "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
        "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
        "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  G_W3 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
        "  G_b3 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
        "  \n",
        "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
        "  \n",
        "  ## GAIN functions\n",
        "  # Generator\n",
        "                \n",
        "  def generator(x,m):\n",
        "    # Concatenate Mask and Data\n",
        "    inputs = tf.concat(values = [x, m], axis = 1) \n",
        "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
        "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
        "    # MinMax normalized output\n",
        "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
        "    return G_prob\n",
        "      \n",
        "  # Discriminator\n",
        "  def discriminator(x, h):\n",
        "    # Concatenate Data and Hint\n",
        "    inputs = tf.concat(values = [x, h], axis = 1) \n",
        "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
        "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
        "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "    return D_prob\n",
        "  \n",
        "  ## GAIN structure\n",
        "  # Generator\n",
        "  G_sample = generator(X, M)\n",
        " \n",
        "  # Combine with observed data\n",
        "  Hat_X = X * M + G_sample * (1-M)\n",
        "  \n",
        "  # Discriminator\n",
        "  D_prob = discriminator(Hat_X, H)\n",
        "  \n",
        "  ## GAIN loss\n",
        "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
        "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
        "  \n",
        "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
        "  \n",
        "  MSE_loss = \\\n",
        "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
        "  \n",
        "  D_loss = D_loss_temp\n",
        "  G_loss = G_loss_temp + alpha * MSE_loss \n",
        "  \n",
        "  ## GAIN solver\n",
        "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
        "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
        "  \n",
        "  ## Iterations\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "   \n",
        "  # Start Iterations\n",
        "  D_loss = []\n",
        "  G_loss_miss = []\n",
        "  G_loss_nonmiss = []\n",
        "  for it in range(iterations):    \n",
        "      \n",
        "    # Sample batch\n",
        "    batch_idx = sample_batch_index(no, batch_size)\n",
        "    X_mb = norm_data_x[batch_idx, :]  \n",
        "    M_mb = data_m[batch_idx, :]  \n",
        "    # Sample random vectors  \n",
        "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
        "    # Sample hint vectors\n",
        "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
        "    H_mb = M_mb * H_mb_temp\n",
        "      \n",
        "    # Combine random vectors with observed vectors\n",
        "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
        "      \n",
        "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
        "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
        "    _, G_loss_curr, MSE_loss_curr = \\\n",
        "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
        "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
        "    #if it % 100 == 0:\n",
        "    #    D_loss.append(D_loss_curr)\n",
        "    #    G_loss_miss.append(G_loss_curr)\n",
        "    #    G_loss_nonmiss.append(MSE_loss_curr)\n",
        "    #if it % 1000 == 0:\n",
        "    #    print('D loss: ' +str(D_loss_curr))\n",
        "    #    print('G loss miss: ' +str(G_loss_curr))\n",
        "    #    print('G loss nonmiss: ' +str(MSE_loss_curr))\n",
        "            \n",
        "  ## Return imputed data      \n",
        "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
        "  M_mb = data_m\n",
        "  X_mb = norm_data_x          \n",
        "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
        "      \n",
        "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
        "  \n",
        "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
        "  \n",
        "  # Renormalization\n",
        "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
        "  \n",
        "  # Rounding\n",
        "          \n",
        "  return imputed_data, D_loss, G_loss_miss, G_loss_nonmiss"
      ],
      "id": "479269a3-0847-4282-bd87-8b62fdd456ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMd4Ik6tsjoS",
        "outputId": "0250cb12-b640-411e-d5c2-9f487541e07e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1614"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Thesis/code/GAIN/570_nomiss/570_12.8_1614.csv')\n",
        "len(df)"
      ],
      "id": "hMd4Ik6tsjoS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fdbc95f-c74b-4419-863a-d624e419502d"
      },
      "outputs": [],
      "source": [
        "ori_data_x, miss_data_x, data_m = data_loader('/content/drive/MyDrive/Thesis/code/GAIN/570_nomiss/570_12.8_1614', 0.2)"
      ],
      "id": "8fdbc95f-c74b-4419-863a-d624e419502d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmME_h7lsyQy"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(miss_data_x.reshape(1614,1), columns=['value']).to_csv('/content/drive/MyDrive/Thesis/code/GAIN/570_nomiss/570_miss.csv')"
      ],
      "id": "rmME_h7lsyQy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "163f9c6b-b274-4752-a056-48bf663ede3f",
        "outputId": "a8563512-6253-453f-874c-2405bb2461e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [10:28<00:00, 125.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35045614616701576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "gain_parameters = {'batch_size': 1,\n",
        "                 'hint_rate': 0.9,\n",
        "                 'alpha': 1,\n",
        "                 'iterations': 20000}\n",
        "\n",
        "rmse_list = []\n",
        "for _ in tqdm(range(5)):\n",
        "  imputed_data_x, D_loss, G_loss_miss, G_loss_nonmiss = gain(miss_data_x, gain_parameters)\n",
        "\n",
        "  # Report the RMSE performance\n",
        "  rmse_list.append(rmse_loss(ori_data_x, imputed_data_x, data_m))\n",
        "\n",
        "print(sum(rmse_list)/len(rmse_list))"
      ],
      "id": "163f9c6b-b274-4752-a056-48bf663ede3f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ukfcK_lIFA",
        "outputId": "60377984-46e3-47c7-8c20-2a6281acf92c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3012484904212392,\n",
              " 0.30254570393683655,\n",
              " 0.29117416625902787,\n",
              " 0.5606858199444662,\n",
              " 0.2966265502735088]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "rmse_list"
      ],
      "id": "-9ukfcK_lIFA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaOvFEs5g_ED"
      },
      "outputs": [],
      "source": [
        "imputed_data_x = imputed_data_x.reshape(1614,1)"
      ],
      "id": "jaOvFEs5g_ED"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBTbdhGusRKD"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(imputed_data_x)[0:50]"
      ],
      "id": "HBTbdhGusRKD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o9JYLdJhW9l"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(imputed_data_x).to_csv('/content/drive/MyDrive/Thesis/code/GAIN/570_nomiss/570_imputed.csv', index=False)"
      ],
      "id": "4o9JYLdJhW9l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f70bc027-d869-47cb-bede-8c87ea4e0917"
      },
      "outputs": [],
      "source": [],
      "id": "f70bc027-d869-47cb-bede-8c87ea4e0917"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}